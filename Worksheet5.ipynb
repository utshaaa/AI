{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#.1.1 Step -1- Data Understanding, Analysis and Preparations:"
      ],
      "metadata": {
        "id": "QBOQ6E-CyDWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-do Task 1"
      ],
      "metadata": {
        "id": "MFDiuL_zyuNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvdkLOmUWi7i",
        "outputId": "30dc326d-3eda-4dd5-9704-02d9f71d9979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "Information about the data:\n",
            "\n",
            "\n",
            " <bound method DataFrame.info of      Math  Reading  Writing\n",
            "0      48       68       63\n",
            "1      62       81       72\n",
            "2      79       80       78\n",
            "3      76       83       79\n",
            "4      59       64       62\n",
            "..    ...      ...      ...\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "[1000 rows x 3 columns]>\n",
            "Description of the data:\n",
            "\n",
            "\n",
            " <bound method NDFrame.describe of      Math  Reading  Writing\n",
            "0      48       68       63\n",
            "1      62       81       72\n",
            "2      79       80       78\n",
            "3      76       83       79\n",
            "4      59       64       62\n",
            "..    ...      ...      ...\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "[1000 rows x 3 columns]>\n",
            "\n",
            "Features (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Label (Y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#1\n",
        "data=pd.read_csv(\"student.csv\")\n",
        "\n",
        "#2\n",
        "#Top and bottom data\n",
        "print(data.head())\n",
        "print(data.tail())\n",
        "\n",
        "#3\n",
        "print(\"Information about the data:\\n\")\n",
        "print(\"\\n\",data.info)\n",
        "\n",
        "#4\n",
        "print(\"Description of the data:\\n\")\n",
        "print(\"\\n\",data.describe)\n",
        "\n",
        "\n",
        "#5\n",
        "X = data[['Math', 'Reading']]  # Features\n",
        "Y = data['Writing']  # Label(Output to be predicted)\n",
        "\n",
        "# Displaying the features and label\n",
        "print(\"\\nFeatures (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\nLabel (Y):\")\n",
        "print(Y.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-do task 2"
      ],
      "metadata": {
        "id": "hgaGNrIgyGo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X_matrix = X.T\n",
        "W = np.random.rand(X_matrix.shape[0], 1)\n",
        "Y_matrix = np.dot(W.T, X_matrix)\n",
        "\n",
        "print(\"\\nResulting Matrix (Y):\")\n",
        "print(Y_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4_5MVXDXN42",
        "outputId": "83ea2d3d-0892-4c75-a180-dc411f858ef9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resulting Matrix (Y):\n",
            "[[58.39799565 70.22814112 71.81340383 73.68336427 56.87097006 73.47846584\n",
            "  73.61650227 42.8947913  67.80603496 71.25694568 61.09186222 76.93800143\n",
            "  41.22972929 70.56676354 61.71518237 40.26347429 62.33850251 42.75675487\n",
            "  78.32267815 67.45447524 53.26908562 58.11761036 64.06611409 48.28683688\n",
            "  74.86314256 77.90856886 58.8077925  64.8317831  70.70479997 71.26557055\n",
            "  68.42504268 78.04660529 65.17471795 63.02437222 39.63584171 52.9304632\n",
            "  47.25371989 68.97718839 65.8649001  39.29721928 76.31468128 69.39129768\n",
            "  56.17647549 44.34633002 60.19246922 64.34218695 47.39175631 65.17471795\n",
            "  88.55893604 64.34649938 89.93930032 77.83739443 70.22814112 45.04082459\n",
            "  44.55554088 44.62240287 50.43287018 44.83161373 63.16240865 71.33243255\n",
            "  50.15679732 81.36379201 88.41658717 73.13121855 41.29659128 82.05397415\n",
            "  59.50228708 76.44409284 55.06787162 53.26908562 55.8292282  53.06418719\n",
            "  43.72300987 56.24764992 57.77036307 46.70157417 59.56914907 39.92485186\n",
            "  69.32012325 59.01269092 73.75022626 70.50421397 62.6857498  68.42504268\n",
            "  40.12975029 58.25133435 76.17664485 70.22382868 69.8097194  72.78397126\n",
            "  46.28315245 65.37961638 63.09985909 64.47591094 58.25564678 60.74892737\n",
            "  53.96789263 56.79979563 87.73071746 78.59443857 28.15725596 47.39175631\n",
            "  67.24957682 47.05313389 35.62847284 70.22382868 68.42073024 39.98740143\n",
            "  69.60050854 41.51442701 61.30107308 71.81771626 78.17601685 79.63618044\n",
            "  42.48068201 73.34042941 49.39975318 67.38761324 78.03798042 68.55876667\n",
            "  62.26732808 71.81340383 74.16864798 60.66912807 42.96596573 43.17086415\n",
            "  33.9720357  49.95189889 63.02005979 76.16801998 68.49190467 69.53795897\n",
            "  60.6065785  50.22797175 50.29483375 71.6753674  73.12259368 73.05573169\n",
            "  79.15089672 76.37723084 57.8415375  86.83132446 63.44279394 61.64400794\n",
            "  58.7409305  73.6876767  61.84459393 88.55893604 69.66737053 53.69181977\n",
            "  55.69119177 75.6201867  63.44279394 51.95558333 55.00100963 42.61871844\n",
            "  78.73678743 65.58882724 55.83785307 65.37961638 73.26925498 59.29738865\n",
            "  36.25179299 66.21645982 63.37161951 65.93176209 78.72816256 65.45079081\n",
            "  61.02500022 86.13251745 52.23596862 67.24957682 58.66544364 53.27339805\n",
            "  64.48453581 80.53557344 76.31036885 56.59058477 67.80603496 56.80410807\n",
            "  60.32619321 52.57890347 59.56483664 56.6617592  60.33481808 72.8551457\n",
            "  68.91032639 74.65824414 73.81708826 77.13858742 52.44086704 81.564378\n",
            "  56.59058477 63.50534351 47.53410518 66.7642931  78.799337   61.15872422\n",
            "  66.55939467 68.28269381 79.28462071 63.16672109 56.87097006 62.05380479\n",
            "  77.48583471 41.44756501 53.61633291 44.00339516 66.41704581 57.01331893\n",
            "  52.36969261 54.85866076 68.55876667 48.98564389 59.98325836 86.13251745\n",
            "  40.54385958 81.6441773  55.21022049 75.96312156 48.7721206  66.14097295\n",
            "  57.90408706 70.56676354 72.44103641 69.67168297 59.08817779 65.24157995\n",
            "  57.21821735 67.93544653 47.94821446 55.90040263 69.18208682 66.28332182\n",
            "  71.05635969 77.35211071 73.19808055 37.15549842 62.61457537 72.50358597\n",
            "  66.07411096 30.78426054 62.6857498  43.99908273 57.49429021 71.81771626\n",
            "  43.8610463  29.26585982 56.6617592  38.46900071 83.30061445 56.31451191\n",
            "  46.56353774 74.23982242 76.03860842 70.57107597 35.49474885 66.28332182\n",
            "  66.41704581 57.07586849 58.18447235 57.56115221 46.00276716 17.08846706\n",
            "  57.56115221 77.20544941 69.05267526 62.20046609 65.17040552 55.00532206\n",
            "  86.89387402 42.75675487 60.25933122 38.19292786 72.22751312 54.23965305\n",
            "  53.06418719 72.50358597 62.05380479 60.74461493 63.85690323 47.46724318\n",
            "  45.17886102 58.66544364 66.7642931  77.28093628 66.27900938 41.91991143\n",
            "  49.54210204 56.04275149 61.29244821 68.70111553 59.63601107 62.61026294\n",
            "  48.98133146 48.84329503 44.41750445 38.60272471 70.15265425 53.47829648\n",
            "  83.85276016 81.56869044 56.93783206 41.78618743 60.74461493 52.43655461\n",
            "  60.6734405  40.88679443 71.26125812 76.17233242 50.43718261 80.67360987\n",
            "  53.48260891 76.38154328 68.70111553 67.04036596 65.72686367 69.60050854\n",
            "  60.53971651 59.15072735 48.29114932 58.38937078 63.09554665 63.43848151\n",
            "  58.52740721 67.93975896 74.71648127 63.03299709 70.28637825 72.29437511\n",
            "  73.89257513 62.8194738  57.9083995  48.98564389 86.61780117 57.49860264\n",
            "  63.85690323 56.10530105 81.50182844 49.67582604 72.6416224  53.41143448\n",
            "  83.57237487 61.37224751 52.9304632  70.49558911 86.6889756  62.61457537\n",
            "  57.83722506 69.52933411 68.63856597 29.26154739 84.7564656  57.6280142\n",
            "  80.80733386 30.44132568 67.10722795 75.34411385 53.68750734 69.25757368\n",
            "  74.6539317  75.40666341 49.11936789 38.80762313 53.13104919 56.38137391\n",
            "  42.8947913  66.8311551  60.25933122 62.33419008 78.38522771 60.18815678\n",
            "  61.64400794 69.52933411 63.37161951 53.96789263 57.49429021 64.75629623\n",
            "  73.95943712 77.06741299 76.72447813 73.26063011 62.89064823 72.23182555\n",
            "  65.09923109 59.42680021 64.41336138 49.74700047 61.99125523 75.41097584\n",
            "  54.8629732  66.14528539 40.748758   74.51158284 50.01876089 50.16110975\n",
            "  68.14465739 66.28332182 76.93800143 59.84522193 88.83500889 57.63232664\n",
            "  66.62625667 56.18078792 60.46422964 72.43672398 66.62625667 43.65183544\n",
            "  59.36425065 55.90040263 82.33004701 83.84844773 58.53171964 71.39929454\n",
            "  57.28076692 50.70894304 36.7370767  71.81340383 85.51350974 65.37961638\n",
            "  56.73724607 56.11392592 69.39129768 43.37576258 58.45623278 87.17425932\n",
            "  45.73531917 36.04689456 72.23613798 79.28462071 66.13666052 35.76650927\n",
            "  67.10291552 72.15633868 49.60896404 66.21214738 77.41897271 84.7564656\n",
            "  63.09554665 35.62847284 77.90425643 74.37354641 48.42487331 84.89018959\n",
            "  64.8986451  71.6753674  74.09316112 50.50835704 62.19184122 53.89240576\n",
            "  47.59665474 60.32619321 64.89433266 53.68750734 41.37207815 75.34411385\n",
            "  40.81993243 78.31836571 65.93607453 61.49734664 56.24764992 44.9071006\n",
            "  44.8273013  43.51379901 58.80348006 74.31099685 62.89064823 69.04405039\n",
            "  57.77036307 78.665613   60.7403025  68.55876667 71.40360698 56.17216305\n",
            "  67.86858453 74.51589527 58.80348006 48.8389826  56.03843906 49.26171675\n",
            "  72.98886969 52.92615076 64.27101252 63.09554665 68.42935511 70.56676354\n",
            "  43.03282773 71.39929454 46.42118888 65.09923109 72.5747604  64.48453581\n",
            "  52.7169399  87.24543375 76.23919441 68.98581326 61.43479708 77.56132157\n",
            "  46.55922531 49.32857875 62.12929165 34.24379612 53.34026005 57.42742821\n",
            "  51.88872133 59.91208393 63.58083037 77.97543086 70.22382868 66.14097295\n",
            "  63.09123422 50.01876089 50.29914618 43.31321302 69.80540696 37.36039685\n",
            "  55.06787162 41.30521615 74.23550998 48.49604774 73.19808055 61.22989865\n",
            "  53.95926776 68.00662096 46.14942846 54.16847862 69.11522482 67.93544653\n",
            "  63.50965594 49.25740432 43.65183544 54.7961112  73.06866899 25.7351498\n",
            "  89.24911818 61.8532188  77.699358   49.67582604 42.13343472 74.02629912\n",
            "  54.30651505 38.67389914 56.51941034 76.38154328 69.80540696 79.91656573\n",
            "  70.70048753 86.69328803 73.95943712 72.23182555 64.82747066 51.19853919\n",
            "  83.09140359 60.26364365 61.78204437 55.7623662  68.56307911 64.47591094\n",
            "  57.21390492 41.02483086 62.89064823 70.56676354 82.18769815 37.49412084\n",
            "  70.15265425 74.44472084 52.23165618 75.61587427 57.21390492 86.62642604\n",
            "  47.04882146 58.94582893 60.8071645  65.51765281 67.38761324 59.22621422\n",
            "  59.22621422 50.78011747 61.64400794 61.6396955  77.42328514 54.8629732\n",
            "  61.36362265 50.57521904 75.34411385 63.37161951 61.64832037 68.21583182\n",
            "  38.18861542 61.02068779 55.07218406 89.93930032 48.07762602 60.25501878\n",
            "  56.45686078 57.35625378 73.88826269 80.80733386 88.41658717 59.8409095\n",
            "  64.62257224 74.79196813 65.23726752 84.12883302 56.38568635 62.96182266\n",
            "  60.74461493 56.59489721 58.11761036 63.02437222 70.7759744  84.12883302\n",
            "  65.72686367 70.90538596 46.69726174 64.06180166 73.47415341 33.20636669\n",
            "  64.68943424 62.33850251 63.30475751 29.88486753 69.7428574  48.08193846\n",
            "  62.47653894 70.63793797 87.79757946 71.95144026 73.13121855 75.62449914\n",
            "  47.39606875 74.09747355 75.34411385 85.30861131 77.96680599 87.6552306\n",
            "  62.47222651 65.72255123 75.75822313 57.21390492 47.66782917 56.31451191\n",
            "  86.41290274 57.42742821 72.7753464  61.99125523 59.7740475  69.88089383\n",
            "  62.26732808 73.26494255 80.18832615 63.37161951 75.13059055 52.44086704\n",
            "  86.20800432 69.18208682 53.27771048 71.8090914  55.96726463 71.6085054\n",
            "  54.44455148 57.35625378 88.97304532 62.8194738  67.45447524 43.51811144\n",
            "  61.92870566 82.46808344 52.50772904 56.59489721 46.27884002 88.14482675\n",
            "  35.07632713 76.72447813 70.7759744  55.83354063 53.82554377 65.03236909\n",
            "  61.57714594 68.21583182 74.78334326 74.16864798 47.66782917 63.30906995\n",
            "  88.69697247 62.74829937 46.8396106  80.87419586 42.54754401 70.78028683\n",
            "  89.93930032 38.05489143 81.42634157 51.06481519 38.94565956 20.9577995\n",
            "  57.49429021 52.09793219 57.63232664 70.91832326 45.6598323  44.35064245\n",
            "  37.15549842 82.60611987 35.49474885 54.65807477 75.76253557 54.03475463\n",
            "  69.46247211 88.14051432 57.00900649 36.32727985 80.87419586 74.44472084\n",
            "  53.75868177 64.82747066 70.42872711 63.7857288  76.45271771 59.49797464\n",
            "  69.80540696 78.59012614 54.44886391 74.23550998 76.93368899 53.62064534\n",
            "  81.85338816 57.63232664 67.18271482 64.48022338 69.11953725 50.29483375\n",
            "  38.53586271 78.80796186 75.96743399 79.97911529 67.18271482 33.96772326\n",
            "  67.45878768 49.53347717 56.7286212  72.29868755 74.99686656 88.97304532\n",
            "  56.6617592  87.6552306  73.81708826 61.50165907 52.57459104 67.93544653\n",
            "  82.6772943  62.96182266 61.6396955  56.80410807 73.67905183 65.51765281\n",
            "  65.58882724 61.50597151 74.02629912 56.31019948 57.07155606 78.38954014\n",
            "  48.84760747 67.04467839 68.56739154 67.94407139 61.15872422 66.42135824\n",
            "  46.35432688 16.26024849 54.03475463 82.53925787 67.10722795 62.05811722\n",
            "  55.96726463 55.69981664 65.58882724 55.8292282  70.56676354 64.89433266\n",
            "  42.33833315 79.565006   54.17279105 67.65937367 75.00117899 56.17216305\n",
            "  42.61871844 27.8143211  67.65937367 50.98932833 58.46054521 65.72255123\n",
            "  78.59012614 60.33050565 72.71279683 75.26862698 61.16303665 46.28315245\n",
            "  89.93930032 57.21390492 54.45317635 55.96726463 64.89002023 51.06050276\n",
            "  72.5747604  46.49236331 50.7844299  52.6500779  58.66544364 43.03282773\n",
            "  63.44279394 77.14289985 68.83915196 50.02738576 54.23534062 77.41466027\n",
            "  71.26125812 54.10161662 61.02500022 49.81386247 81.01223229 65.24589238\n",
            "  54.92983519 86.89818646 60.95813823 58.05074836 65.30844195 71.19008368\n",
            "  55.8292282  51.26971362 89.93930032 52.51635391 50.02307333 61.78204437\n",
            "  75.33980141 54.37337705 76.79565256 71.53733097 57.21821735 78.72816256\n",
            "  72.44103641 65.38392881 60.46854208 66.96919152 44.96965016 41.09600529\n",
            "  64.41336138 46.49236331 70.70479997 78.799337   34.73339227 54.16847862\n",
            "  74.85451769 83.09140359 70.09010469 57.56115221 62.05811722 71.6753674\n",
            "  49.11936789 55.06787162 66.6974311  72.6416224  58.11329792 60.46854208\n",
            "  61.92008079 39.715641   63.92376523 57.9083995  51.47461204 68.14465739\n",
            "  53.47829648 68.00662096 73.12690612 65.31275438 53.68750734 65.65568924\n",
            "  89.11108175 82.6772943  78.25150371 63.58514281 56.38568635 40.74444557\n",
            "  49.18622989 59.57346151 76.79565256 57.21821735 59.98325836 66.8311551\n",
            "  41.64815101 80.25518815 53.54515847 74.16433555 62.61457537 62.88633579\n",
            "  41.02914329 58.18447235 60.39736764 62.61457537 72.50789841 51.60833604\n",
            "  66.96919152 65.7980381  76.17233242 46.42118888 85.86075703 87.72640503\n",
            "  39.36408128 42.27578359 82.40122144 71.18577125 64.19983808 51.19853919\n",
            "  76.72879057 70.70479997 59.28876378 36.10944412 56.24764992 56.03843906\n",
            "  61.15872422 45.45924631 58.73661807 55.76667863 75.48215027 70.49990154\n",
            "  72.23613798 67.11154039 51.95989576 75.48215027 44.61809044 59.15503979\n",
            "  83.30061445 71.88457826 33.69165041 84.5429423  43.78987187 66.48822024\n",
            "  73.12690612 67.03605352 54.30651505 74.16864798 74.16433555 63.02868466\n",
            "  64.20415052 60.60226607 61.23421108 77.97543086 53.96358019 40.53954714\n",
            "  50.70894304 76.72879057 38.949972   50.98932833 56.46117321 82.6772943\n",
            "  77.20976185 55.21022049 58.66544364 59.15503979 50.9181539  69.04836282\n",
            "  69.52933411 73.8214007  63.92376523 59.35993821 62.68143737 59.98325836\n",
            "  60.6734405  64.48022338 57.8415375  81.98711216 61.8532188  65.99862409\n",
            "  69.18639925 49.95621133 63.37593195 57.35625378 72.71279683 72.43672398\n",
            "  60.81147693 75.06372855 75.20607742 47.32489432 64.6182598  54.51572591\n",
            "  55.96726463 68.6968031  59.22190178 56.31882435 57.56115221 81.63986487\n",
            "  52.09793219 47.31626945 85.24174931 57.56115221 47.59665474 66.27900938\n",
            "  75.55332471 78.52326414 73.88826269 59.35993821]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Split the dataset\n",
        "# Perform an 80-20 split (80% training, 20% testing)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Display the shapes of the resulting splits\n",
        "print(\"Training Feature Matrix (X_train):\", X_train.shape)\n",
        "print(\"Test Feature Matrix (X_test):\", X_test.shape)\n",
        "print(\"Training Label Vector (Y_train):\", Y_train.shape)\n",
        "print(\"Test Label Vector (Y_test):\", Y_test.shape)\n"
      ],
      "metadata": {
        "id": "CoEWe1q8YBL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75006f4d-c400-47cf-91e0-dcbaca478c97"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Feature Matrix (X_train): (80, 3)\n",
            "Test Feature Matrix (X_test): (20, 3)\n",
            "Training Label Vector (Y_train): (80,)\n",
            "Test Label Vector (Y_test): (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.2 Step -2- Build a Cost Function:"
      ],
      "metadata": {
        "id": "TRvbeQAy5PrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X, Y, W):\n",
        "    Y_pred = Y_matrix # Shape: (n_samples,)\n",
        "\n",
        "    # Step 2: Compute errors (Y_pred - Y)\n",
        "    errors = Y_pred - Y  # Shape: (n_samples,)\n",
        "\n",
        "    # Step 3: Compute Mean Squared Error\n",
        "    n = X.shape[0]  # Number of training examples\n",
        "    cost = np.sum(errors**2) / (2 * n)\n",
        "    return cost\n",
        "X_test = np.array([[1, 3, 5],  # Feature Matrix (rows = samples, cols = features)\n",
        "                   [2, 4, 6]]).T  # Transpose to match shape (3, 2)\n",
        "Y_test = np.array([3, 7, 11])  # Target Vector\n",
        "W_test = np.array([1, 1])  # Weight Vector\n",
        "\n",
        "# Compute cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "# Verify the output\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"Something went wrong: Reimplement the cost function\")\n",
        "print(\"Cost function output:\", cost)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRgqBYWM5V4s",
        "outputId": "0c30f964-88d8-4a6e-9d7a-aae2e0ef0e6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.3 Step -3- Gradient Descent for Simple Linear Regression:"
      ],
      "metadata": {
        "id": "Yoo5tWmJ84J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    W_update = W\n",
        "# Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    W_update = W\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values (Y_pred = X · W)\n",
        "        Y_pred = np.dot(X, W_update)  # Shape: (m x 1)\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y (Loss)\n",
        "        loss = Y_pred - Y  # Shape: (m x 1)\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)  # Shape: (n x 1)\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W_update - alpha * dw  # Shape: (n x 1)\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)  # Compute the new cost\n",
        "        cost_history[iteration] = cost  # Store cost in history\n",
        "\n",
        "    return W_update, cost_history\n",
        "np.random.seed(0)\n",
        "X = np.random.rand(100, 3)\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3)\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Test the gradient_descent function\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Print the final parameters and cost history\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Cost History:\", cost_history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlY0NKtX88-k",
        "outputId": "34449f9a-f093-4746-c8ea-a88d46934a14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Cost History: [0.10711197094660153, 0.10634880599939901, 0.10559826315680616, 0.10486012948320558, 0.1041341956428534, 0.10342025583900626, 0.1027181077540776, 0.1020275524908062, 0.10134839451441931, 0.1006804415957737, 0.1000235047554587, 0.09937739820884377, 0.09874193931205609, 0.09811694850887098, 0.09750224927850094, 0.0968976680842672, 0.09630303432313951, 0.09571818027612913, 0.09514294105952065, 0.09457715457692842, 0.09402066147216397, 0.09347330508290015, 0.09293493139511913, 0.09240538899833017, 0.09188452904154543, 0.0913722051899995, 0.09086827358260123, 0.09037259279010502, 0.08988502377398917, 0.08940542984603007, 0.08893367662855953, 0.08846963201539432, 0.08801316613342668, 0.08756415130486386, 0.08712246201010665, 0.08668797485125507, 0.08626056851623205, 0.08584012374351278, 0.08542652328745133, 0.08501965188419301, 0.0846193962181636, 0.08422564488912489, 0.08383828837978763, 0.08345721902397185, 0.08308233097530582, 0.08271352017645425, 0.08235068432886682, 0.08199372286303816, 0.08164253690927113, 0.08129702926893387, 0.08095710438620353, 0.08062266832028739, 0.08029362871811391, 0.07996989478748553, 0.0796513772706855, 0.07933798841853087, 0.07902964196486459, 0.07872625310147845, 0.07842773845346054, 0.07813401605495937, 0.0778450053253578, 0.0775606270458499, 0.07728080333641404, 0.07700545763317514, 0.07673451466614989, 0.07646790043736812, 0.07620554219936448, 0.07594736843403344, 0.07569330883184205, 0.07544329427139428, 0.07519725679934072, 0.0749551296106282, 0.07471684702908327, 0.07448234448832412, 0.0742515585129952, 0.07402442670031911, 0.0738008877019607, 0.07358088120619749, 0.0733643479203919, 0.07315122955375959, 0.07294146880042966, 0.07273500932279067, 0.07253179573511871, 0.07233177358748233, 0.0721348893499193, 0.07194109039688139, 0.07175032499194182, 0.07156254227276149, 0.07137769223630935, 0.07119572572433286, 0.07101659440907385, 0.07084025077922623, 0.070666648126131, 0.07049574053020462, 0.07032748284759716, 0.07016183069707572, 0.0699987404471299, 0.06983816920329523, 0.06968007479569092, 0.06952441576676843, 0.06937115135926715, 0.06922024150437375, 0.06907164681008185, 0.06892532854974835, 0.0687812486508435, 0.06863936968389095, 0.06849965485159508, 0.06836206797815195, 0.06822657349874123, 0.06809313644919561, 0.067961722455845, 0.06783229772553254, 0.06770482903579932, 0.06757928372523506, 0.06745562968399212, 0.06733383534445969, 0.06721386967209597, 0.067095702156415, 0.06697930280212627, 0.06686464212042395, 0.06675169112042348, 0.0666404213007429, 0.06653080464122665, 0.06642281359480932, 0.06631642107951677, 0.06621160047060279, 0.06610832559281864, 0.06600657071281309, 0.0659063105316614, 0.06580752017752023, 0.06571017519840698, 0.06561425155510119, 0.06551972561416586, 0.06542657414108709, 0.06533477429352925, 0.06524430361470467, 0.06515514002685512, 0.06506726182484374, 0.06498064766985515, 0.06489527658320228, 0.06481112794023773, 0.06472818146436811, 0.0646464172211699, 0.06456581561260431, 0.06448635737133043, 0.0644080235551142, 0.06433079554133217, 0.06425465502156798, 0.06417958399630046, 0.06410556476968135, 0.06403257994440141, 0.0639606124166433, 0.06388964537111992, 0.06381966227619645, 0.06375064687909507, 0.06368258320118075, 0.06361545553332655, 0.06354924843135755, 0.06348394671157162, 0.06341953544633615, 0.06335599995975896, 0.06329332582343267, 0.06323149885225086, 0.06317050510029515, 0.06311033085679153, 0.06305096264213547, 0.06299238720398384, 0.0629345915134133, 0.06287756276114324, 0.06282128835382297, 0.0627657559103815, 0.06271095325843898, 0.06265686843077901, 0.06260348966188052, 0.06255080538450809, 0.06249880422636036, 0.06244747500677472, 0.06239680673348793, 0.06234678859945137, 0.06229740997970036, 0.06224866042827619, 0.06220052967520031, 0.062153007623499706, 0.062106084346282515, 0.06205975008386309, 0.06201399524093575, 0.06196881038379625, 0.061924186237610215, 0.061880113683727866, 0.0618365837570441, 0.06179358764340313, 0.061751116677047156, 0.06170916233810801, 0.0616677162501414, 0.06162677017770278, 0.061586316023964055, 0.0615463458283708, 0.06150685176433905, 0.06146782613699094, 0.0614292613809287, 0.061391150058046254, 0.06135348485537794, 0.06131625858298352, 0.061279464171868706, 0.06124309467194143, 0.061207143250002184, 0.0611716031877684, 0.06113646787993252, 0.061101730832252524, 0.06106738565967507, 0.06103342608449018, 0.06099984593451716, 0.06096663914132128, 0.0609337997384604, 0.0609013218597616, 0.06086919973762659, 0.06083742770136588, 0.06080600017556133, 0.06077491167845611, 0.06074415682037193, 0.060713730302153254, 0.060683626913637524, 0.060653841532151406, 0.060624369121032556, 0.0605952047281761, 0.06056634348460599, 0.060537780603070336, 0.06050951137666054, 0.0604815311774538, 0.060453835455178496, 0.06042641973590228, 0.06039927962074215, 0.060372410784596583, 0.060345808974898815, 0.06031947001039151, 0.06029338977992186, 0.06026756424125725, 0.060241989419920934, 0.06021666140804729, 0.0601915763632565, 0.06016673050754826, 0.060142120126214255, 0.06011774156676883, 0.06009359123789796, 0.06006966560842588, 0.06004596120629915, 0.060022474617588105, 0.059999202485504784, 0.059976141509438, 0.0599532884440042, 0.05993064009811483, 0.05990819333405906, 0.059885945066602345, 0.059863892262100066, 0.059842031937626106, 0.059820361160116395, 0.05979887704552664, 0.05977757675800453, 0.05975645750907579, 0.05973551655684408, 0.0597147512052044, 0.05969415880306974, 0.05967373674361096, 0.05965348246350928, 0.05963339344222168, 0.059613467201258485, 0.059593701303473294, 0.05957409335236496, 0.0595546409913911, 0.05953534190329372, 0.05951619380943562, 0.0594971944691485, 0.0594783416790919, 0.05945963327262296, 0.0594410671191769, 0.05942264112365792, 0.05940435322584049, 0.059386201399780576, 0.059368183653237094, 0.059350298027102844, 0.05933254259484532, 0.05931491546195686, 0.05929741476541398, 0.0592800386731462, 0.05926278538351338, 0.05924565312479226, 0.05922864015467153, 0.059211744759755505, 0.05919496525507604, 0.05917829998361292, 0.05916174731582211, 0.059145305649172315, 0.059128973407688926, 0.05911274904150609, 0.05909663102642617, 0.05908061786348662, 0.059064708078534194, 0.05904890022180654, 0.05903319286752055, 0.05901758461346795, 0.05900207408061755, 0.058986659912724324, 0.05897134077594504, 0.058956115358460404, 0.05894098237010357, 0.05892594054199501, 0.05891098862618344, 0.05889612539529293, 0.05888134964217588, 0.05886666017957195, 0.058852055839772675, 0.05883753547429179, 0.05882309795354117, 0.058808742166512155, 0.05879446702046235, 0.058780271440607684, 0.058766154369819606, 0.05875211476832761, 0.05873815161342641, 0.05872426389918856, 0.058710450636181515, 0.05869671085118971, 0.058683043586941104, 0.058669447901838714, 0.05865592286969638, 0.05864246757947903, 0.05862908113504752, 0.05861576265490756, 0.058602511271963004, 0.05858932613327336, 0.058576206399815284, 0.05856315124624814, 0.058550159860683564, 0.058537231444458875, 0.05852436521191438, 0.05851156039017436, 0.0584988162189318, 0.05848613195023677, 0.05847350684828838, 0.058460940189230176, 0.05844843126094919, 0.05843597936287806, 0.05842358380580092, 0.05841124391166213, 0.058398959013378576, 0.05838672845465502, 0.05837455158980245, 0.05836242778355972, 0.05835035641091811, 0.05833833685694878, 0.05832636851663321, 0.05831445079469654, 0.058302583105443666, 0.05829076487259809, 0.05827899552914358, 0.05826727451716844, 0.058255601287712386, 0.0582439753006161, 0.058232396024373266, 0.05822086293598511, 0.058209375520817355, 0.058197933272459756, 0.05818653569258779, 0.05817518229082684, 0.058163872584618616, 0.05815260609908985, 0.058141382366923164, 0.058130200928230166, 0.058119061330426776, 0.05810796312811039, 0.05809690588293942, 0.05808588916351474, 0.058074912545263056, 0.058063975610322414, 0.058053077947429504, 0.05804221915180897, 0.05803139882506447, 0.05802061657507169, 0.0580098720158732, 0.05799916476757483, 0.057988494456244134, 0.057977860713810364, 0.057967263177966154, 0.05795670149207094, 0.05794617530505593, 0.05793568427133074, 0.057925228050691516, 0.057914806308230836, 0.057904418714248757, 0.057894064944165734, 0.0578837446784368, 0.05787345760246728, 0.05786320340652982, 0.05785298178568306, 0.05784279243969133, 0.057832635072946, 0.05782250939438805, 0.0578124151174319, 0.05780235195989063, 0.057792319643902336, 0.05778231789585793, 0.0577723464463298, 0.05776240503000217, 0.05775249338560223, 0.05774261125583255, 0.05773275838730473, 0.05772293453047407, 0.057713139439575324, 0.057703372872559694, 0.05769363459103265, 0.057683924360193005, 0.05767424194877295, 0.05766458712897906, 0.05765495967643433, 0.057645359370121226, 0.05763578599232564, 0.057626239328581796, 0.05761671916761811, 0.057607225301304, 0.05759775752459752, 0.057588315635493874, 0.057578899434974906, 0.05756950872695933, 0.057560143318253806, 0.05755080301850501, 0.057541487640152184, 0.05753219699838088, 0.057522930911077144, 0.05751368919878268, 0.05750447168465076, 0.05749527819440267, 0.057486108556285255, 0.05747696260102877, 0.05746784016180581, 0.05745874107419066, 0.05744966517611951, 0.057440612307851226, 0.05743158231192885, 0.05742257503314173, 0.057413590318488285, 0.05740462801713937, 0.05739568798040233, 0.057386770061685605, 0.05737787411646401, 0.05736900000224435, 0.05736014757853204, 0.05735131670679789, 0.057342507250445686, 0.05733371907478018, 0.05732495204697581, 0.05731620603604562, 0.057307480912811105, 0.057298776549872255, 0.05729009282157824, 0.05728142960399854, 0.05727278677489465, 0.05726416421369212, 0.05725556180145319, 0.05724697942084986, 0.0572384169561373, 0.05722987429312795, 0.05722135131916572, 0.05721284792310102, 0.05720436399526589, 0.0571958994274496, 0.057187454112874896, 0.057179027946174334, 0.05717062082336728, 0.057162232641836994, 0.05715386330030848, 0.05714551269882637, 0.05713718073873334, 0.05712886732264895, 0.05712057235444866, 0.05711229573924336, 0.05710403738335914, 0.0570957971943175, 0.057087575080815786, 0.05707937095270803, 0.057071184720986066, 0.05706301629776107, 0.05705486559624517, 0.057046732530733744, 0.057038617016587564, 0.05703051897021566, 0.05702243830905817, 0.057014374951569684, 0.057006328817202634, 0.05699829982639134, 0.05699028790053585, 0.05698229296198645, 0.05697431493402824, 0.056966353740865984, 0.05695840930760929, 0.056950481560257914, 0.0569425704256875, 0.0569346758316353, 0.05692679770668645, 0.05691893598026014, 0.05691109058259633, 0.05690326144474244, 0.05689544849854041, 0.056887651676613984, 0.05687987091235604, 0.056872106139916355, 0.05686435729418944, 0.05685662431080263, 0.0568489071261043, 0.05684120567715238, 0.056833519901703065, 0.05682584973819958, 0.05681819512576124, 0.05681055600417275, 0.056802932313873525, 0.056795323995947306, 0.056787730992111936, 0.05678015324470926, 0.05677259069669528, 0.05676504329163035, 0.05675751097366966, 0.05674999368755382, 0.05674249137859953, 0.05673500399269066, 0.05672753147626912, 0.056720073776326194, 0.05671263084039382, 0.056705202616536096, 0.05669778905334098, 0.05669039009991206, 0.05668300570586036, 0.05667563582129657, 0.056668280396823104, 0.05666093938352648, 0.05665361273296975, 0.056646300397185066, 0.05663900232866641, 0.05663171848036241, 0.05662444880566922, 0.05661719325842369, 0.056609951792896414, 0.056602724363785134, 0.05659551092620811, 0.05658831143569758, 0.05658112584819342, 0.05657395412003692, 0.05656679620796451, 0.05655965206910183, 0.05655252166095763, 0.056545404941418, 0.056538301868740586, 0.05653121240154893, 0.056524136498826864, 0.056517074119913024, 0.056510025224495546, 0.05650298977260663, 0.05649596772461748, 0.056488959041233064, 0.05648196368348717, 0.05647498161273735, 0.05646801279066009, 0.056461057179246134, 0.05645411474079556, 0.05644718543791332, 0.05644026923350467, 0.056433366090770515, 0.05642647597320331, 0.05641959884458242, 0.05641273466897008, 0.05640588341070717, 0.05639904503440896, 0.05639221950496121, 0.05638540678751615, 0.05637860684748858, 0.056371819650551894, 0.05636504516263454, 0.05635828334991602, 0.05635153417882347, 0.056344797616027884, 0.056338073628440656, 0.05633136218321008, 0.056324663247717996, 0.05631797678957624, 0.05631130277662355, 0.05630464117692215, 0.056297991958754665, 0.05629135509062088, 0.05628473054123466, 0.05627811827952098, 0.05627151827461283, 0.0562649304958483, 0.05625835491276777, 0.05625179149511093, 0.056245240212814004, 0.05623870103600713, 0.05623217393501148, 0.05622565888033667, 0.05621915584267811, 0.05621266479291449, 0.056206185702105234, 0.05619971854148787, 0.05619326328247578, 0.05618681989665565, 0.05618038835578517, 0.056173968631790624, 0.05616756069676472, 0.056161164522964185, 0.05615478008280768, 0.05614840734887347, 0.05614204629389748, 0.056135696890770956, 0.0561293591125386, 0.056123032932396344, 0.05611671832368947, 0.05611041525991056, 0.05610412371469757, 0.05609784366183199, 0.05609157507523687, 0.05608531792897493, 0.05607907219724691, 0.056072837854389615, 0.05606661487487427, 0.056060403233304724, 0.056054202904415734, 0.05604801386307135, 0.056041836084263226, 0.05603566954310899, 0.05602951421485069, 0.05602337007485318, 0.05601723709860265, 0.05601111526170498, 0.05600500453988435, 0.05599890490898177, 0.05599281634495359, 0.055986738823870105, 0.055980672321914095, 0.055974616815379595, 0.055968572280670384, 0.05596253869429871, 0.05595651603288404, 0.05595050427315166, 0.0559445033919315, 0.05593851336615684, 0.055932534172863126, 0.05592656578918666, 0.05592060819236353, 0.05591466135972839, 0.05590872526871329, 0.05590279989684662, 0.05589688522175184, 0.055890981221146614, 0.05588508787284151, 0.055879205154739084, 0.05587333304483278, 0.05586747152120588, 0.055861620562030555, 0.05585578014556684, 0.05584995025016162, 0.05584413085424776, 0.05583832193634303, 0.055832523475049294, 0.0558267354490515, 0.05582095783711688, 0.05581519061809395, 0.05580943377091164, 0.05580368727457861, 0.05579795110818212, 0.055792225250887444, 0.055786509681936956, 0.055780804380649245, 0.05577510932641847, 0.05576942449871345, 0.055763749877076975, 0.05575808544112503, 0.05575243117054599, 0.05574678704509999, 0.05574115304461813, 0.055735529149001775, 0.055729915338221844, 0.05572431159231819, 0.05571871789139883, 0.05571313421563932, 0.05570756054528211, 0.05570199686063586, 0.055696443142074864, 0.055690899370038335, 0.05568536552502988, 0.05567984158761686, 0.055674327538429685, 0.05566882335816142, 0.055663329027567085, 0.055657844527463085, 0.05565236983872667, 0.05564690494229538, 0.0556414498191665, 0.05563600445039652, 0.055630568817100635, 0.05562514290045211, 0.05561972668168197, 0.05561432014207832, 0.05560892326298588, 0.05560353602580557, 0.055598158411993996, 0.055592790403062906, 0.055587431980578784, 0.055582083126162425, 0.05557674382148841, 0.055571414048284674, 0.05556609378833211, 0.055560783023464094, 0.05555548173556606, 0.0555501899065751, 0.05554490751847952, 0.05553963455331848, 0.05553437099318152, 0.055529116820208266, 0.05552387201658792, 0.055518636564558965, 0.05551341044640875, 0.05550819364447313, 0.05550298614113609, 0.05549778791882936, 0.05549259896003212, 0.05548741924727061, 0.05548224876311773, 0.055477087490192804, 0.0554719354111612, 0.055466792508733924, 0.05546165876566746, 0.055456534164763226, 0.05545141868886745, 0.05544631232087083, 0.05544121504370806, 0.055436126840357744, 0.055431047693841926, 0.05542597758722593, 0.055420916503617974, 0.05541586442616892, 0.055410821338071965, 0.05540578722256242, 0.05540076206291734, 0.055395745842455345, 0.05539073854453634, 0.055385740152561175, 0.05538075064997147, 0.055375770020249314, 0.05537079824691705, 0.055365835313536935, 0.05536088120371103, 0.05535593590108084, 0.055350999389327124, 0.055346071652169704, 0.0553411526733671, 0.05533624243671647, 0.05533134092605322, 0.055326448125250914, 0.05532156401822096, 0.05531668858891247, 0.055311821821311946, 0.055306963699443185, 0.05530211420736701, 0.055297273329180996, 0.05529244104901939, 0.0552876173510529, 0.05528280221948834, 0.05527799563856866, 0.055273197592572584, 0.05526840806581448, 0.055263627042644155, 0.055258854507446706, 0.05525409044464235, 0.05524933483868613, 0.05524458767406786, 0.05523984893531189, 0.055235118606976955, 0.05523039667365595, 0.05522568311997588, 0.055220977930597534, 0.05521628109021546, 0.05521159258355772, 0.055206912395385714, 0.055202240510494154, 0.05519757691371069, 0.055192921589895944, 0.05518827452394329, 0.05518363570077869, 0.05517900510536053, 0.05517438272267951, 0.055169768537758505, 0.055165162535652366, 0.055160564701447826, 0.05515597502026334, 0.055151393477248956, 0.05514682005758613, 0.055142254746487665, 0.055137697529197546, 0.0551331483909908, 0.055128607317173346, 0.055124074293081866, 0.055119549304083755, 0.05511503233557687, 0.05511052337298952, 0.05510602240178027, 0.05510152940743782, 0.05509704437548093, 0.055092567291458276, 0.05508809814094826, 0.05508363690955906, 0.055079183582928334, 0.05507473814672324, 0.055070300586640204, 0.05506587088840496, 0.05506144903777224, 0.05505703502052585, 0.055052628822478474, 0.05504823042947156, 0.05504383982737522, 0.05503945700208816, 0.05503508193953754, 0.055030714625678864, 0.05502635504649593, 0.05502200318800065, 0.055017659036233034, 0.055013322577261034, 0.055008993797180404, 0.05500467268211479, 0.05500035921821539, 0.054996053391661005, 0.05499175518865794, 0.054987464595439836, 0.054983181598267664, 0.0549789061834296, 0.05497463833724084, 0.05497037804604376, 0.0549661252962075, 0.054961880074128146, 0.05495764236622849, 0.054953412158958034, 0.054949189438792796, 0.05494497419223534, 0.05494076640581462, 0.05493656606608597, 0.05493237315963089, 0.05492818767305708, 0.05492400959299837, 0.05491983890611449, 0.054915675599091204, 0.05491151965864005, 0.05490737107149833, 0.05490322982442909, 0.054899095904220915, 0.05489496929768798, 0.05489084999166989, 0.05488673797303166, 0.054882633228663574, 0.054878535745481176, 0.054874445510425175, 0.05487036251046136, 0.054866286732580524, 0.054862218163798465, 0.05485815679115577, 0.0548541026017179, 0.054850055582575, 0.05484601572084191, 0.0548419830036581, 0.054837957418187463, 0.05483393895161846, 0.0548299275911639, 0.054825923324060916, 0.05482192613757092, 0.05481793601897949, 0.05481395295559637, 0.05480997693475535, 0.05480600794381422, 0.05480204597015469, 0.05479809100118241, 0.05479414302432678, 0.054790202027040935, 0.05478626799680178, 0.05478234092110976, 0.05477842078748891, 0.0547745075834868, 0.054770601296674395, 0.05476670191464608, 0.05476280942501957, 0.054758923815435796, 0.05475504507355896, 0.05475117318707636, 0.0547473081436984, 0.05474344993115854, 0.0547395985372132, 0.054735753949641676, 0.0547319161562462, 0.05472808514485178, 0.054724260903306156, 0.05472044341947975, 0.054716632681265726, 0.05471282867657969, 0.0547090313933599, 0.054705240819567, 0.05470145694318413, 0.05469767975221677, 0.05469390923469267, 0.05469014537866194, 0.05468638817219682, 0.05468263760339178, 0.05467889366036329, 0.05467515633125, 0.05467142560421251, 0.05466770146743334, 0.054663983909116975, 0.054660272917489705, 0.05465656848079964, 0.05465287058731666, 0.05464917922533233, 0.05464549438315984, 0.054641816049134054, 0.054638144211611325, 0.05463447885896954, 0.05463081997960807, 0.05462716756194763, 0.05462352159443038, 0.054619882065519716, 0.054616248963700355, 0.05461262227747823, 0.05460900199538041, 0.054605388105955124, 0.054601780597771675, 0.05459817945942039, 0.05459458467951261, 0.05459099624668059, 0.05458741414957748, 0.0545838383768773, 0.054580268917274875, 0.05457670575948582, 0.05457314889224635, 0.054569598304313474, 0.0545660539844648, 0.054562515921498494, 0.05455898410423328, 0.054555458521508345, 0.05455193916218337, 0.05454842601513845, 0.05454491906927401, 0.05454141831351079, 0.054537923736789846, 0.054534435328072464, 0.0545309530763401, 0.054527476970594374, 0.05452400699985704, 0.05452054315316988, 0.05451708541959473, 0.054513633788213396, 0.054510188248127645, 0.05450674878845912, 0.05450331539834934, 0.054499888066959656, 0.05449646678347117, 0.054493051537084725, 0.05448964231702087, 0.05448623911251984, 0.05448284191284145, 0.054479450707265106, 0.05447606548508972, 0.054472686235633755, 0.054469312948235114, 0.0544659456122511, 0.05446258421705838, 0.05445922875205301, 0.05445587920665035, 0.05445253557028493, 0.05444919783241064, 0.05444586598250043, 0.054442540010046496, 0.05443921990456002, 0.0544359056555714, 0.054432597252629965, 0.05442929468530409, 0.054425997943181044, 0.05442270701586707, 0.05441942189298729, 0.05441614256418564, 0.05441286901912488, 0.05440960124748651, 0.054406339238970806, 0.05440308298329671, 0.054399832470201845, 0.054396587689442416, 0.05439334863079324, 0.05439011528404767, 0.054386887639017584, 0.054383665685533336, 0.0543804494134437, 0.054377238812615865, 0.05437403387293539, 0.054370834584306166, 0.05436764093665037, 0.054364452919908414, 0.05436127052403898, 0.05435809373901896, 0.05435492255484332]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.4 Step -4- Evaluate the Model:"
      ],
      "metadata": {
        "id": "M0UOykJFB5rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(Y, Y_pred):\n",
        "    squared_diff = (Y - Y_pred) ** 2\n",
        "\n",
        "    # Taking the mean of the squared differences\n",
        "    mean_squared_diff = np.mean(squared_diff)\n",
        "\n",
        "    # Taking the square root of the mean squared difference\n",
        "    rmse = np.sqrt(mean_squared_diff)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    mean_y = np.mean(Y)\n",
        "\n",
        "    # Total Sum of Squares (SST)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "\n",
        "    # Sum of Squared Residuals (SSR)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    # R-squared calculation\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    return r2\n"
      ],
      "metadata": {
        "id": "VUR5fUjrB7Wz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1.5 Step -5- Main Function to Integrate All Steps:"
      ],
      "metadata": {
        "id": "0nj6yqIrCeU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values  # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights (for 2 features: Math and Reading)\n",
        "    alpha = 0.00001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)  # Linear model prediction\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f77SVPE2CfYa",
        "outputId": "583b58fe-2192-44d8-9f5e-aa76ed82b871"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [2013.165570783755, 1640.286832599692, 1337.0619994901585, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951635, 398.7634463599484, 327.4517147324688]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Did your Model Overfitt, Underfitts, or performance is acceptable.\n",
        "\n",
        "=> My model underfitts since RMSE is much higher and r2 is low\n",
        "\n",
        "2. Experiment with different value of learning rate, making it higher and lower, observe the result.\n",
        "\n",
        "=> Lower learning rates gradually decreases the cost and slows convergence but could result in more stable and accurate results. However, higher learning rates may cause the model to converge too quicky."
      ],
      "metadata": {
        "id": "P4XHbDZsGBBr"
      }
    }
  ]
}